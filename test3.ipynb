{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b9f0b37-fbfe-4e59-ab32-da1c1afe3b00",
   "metadata": {},
   "source": [
    "* see: https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75812374-a9a1-4463-93a2-6dd6f352cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# removed OPenAI, using HF\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "# removed OpenAI, using Cohere\n",
    "from langchain.chat_models import ChatCohere\n",
    "\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# private configurations\n",
    "from config_private import COHERE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988c589-f06e-49f7-b23f-f70ab316e720",
   "metadata": {},
   "source": [
    "#### Loading the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5edf3f-17df-4963-82ef-d4297239dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEB_PAGES = [\n",
    "    \"https://luigi-saetta.medium.com/generative-ai-how-to-control-the-creativity-of-your-large-language-model-c7b0322b4c3d\",\n",
    "    \"https://medium.com/the-modern-scientist/a-conversation-around-generative-ai-cca5c408ead4\",\n",
    "]\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(WEB_PAGES)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd8887b-0aff-45b8-9df2-ea904815ea26",
   "metadata": {},
   "source": [
    "#### Splitting the document in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f53344e-271e-4484-bd29-c95a02b35d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 512\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\n",
    "\n",
    "splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89b1b9f1-d375-4a68-b25a-633930c2462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 42 splits...\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {len(splits)} splits...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f70cce26-c603-4532-8ab6-8c34fa4d857e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='trained to generate text.In this article, I‚Äôll discuss some general characteristics of LLM for text generation. Especially those parameters that can be used, in the inference phase, to control the variety of possible answers to prompts. Then, I‚Äôll add some more details regarding Oracle OCI AI Generative Service, which is based on LLMs provided by Cohere.First of all, some basic concepts: how does an LLM generating text work, what is a prompt, and what does it mean that an LLM can be creative?An LLM for', metadata={'source': 'https://luigi-saetta.medium.com/generative-ai-how-to-control-the-creativity-of-your-large-language-model-c7b0322b4c3d', 'title': 'Generative AI: how to control the creativity of your Large Language Model | by Luigi Saetta | Oct, 2023 | Medium', 'description': 'At the heart of every Generative Service for Text, there is a Large Language Model (LLM), a model that has been trained, on a very large corpus of documents, to learn the structure and‚Ä¶', 'language': 'en'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a63cc21-2225-4e1f-8b3a-6ec41ca97d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have substitued OpenAI with HF\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "\n",
    "\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL_NAME, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# using Chroma as Vector store\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=hf)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aaa4a5-ba64-4222-a65c-d26e5395cd3f",
   "metadata": {},
   "source": [
    "#### Define the prompt structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426d2dd3-ac41-4cb2-a975-b35ca482fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2ef0b-f63b-4138-8bd9-5271e66ee926",
   "metadata": {},
   "source": [
    "#### Define the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d5f0d42-8203-4950-a5ca-8c051ff71b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = COHERE_API_KEY\n",
    "\n",
    "llm = ChatCohere(cohere_api_key=API_KEY, temperature=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe0dca-b88e-4c5f-9b03-25da1ed36cb2",
   "metadata": {},
   "source": [
    "#### Define the (Lang)Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e93a1de-9939-4120-8910-66d735de7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | rag_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce71ae-50a8-418d-81ea-cea0a30d11cf",
   "metadata": {},
   "source": [
    "#### Process the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "069689c8-4786-455b-aee0-91f1fc69e0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Large Language Model is a type of AI that is trained on massive amounts of data and designed to generate human-like responses to user requests. There are a plethora of ways to use them, for example, the Large Language Models can be used to generate text. This is one of many possible uses. \n",
      "\n",
      "Is this helpful?\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"\"\"What is a Large Language Model (LLM)? Could you give some examples of what can we do with a LLM? \n",
    "Imagine that we’re talking to a eleven years old boy\"\"\"\n",
    "\n",
    "response = rag_chain.invoke(QUESTION)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93ffe1-3036-4251-9514-0c3cc240f224",
   "metadata": {},
   "source": [
    "#### Explore the vectore store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e055f1c4-27d9-465a-b0b4-278ad06bc67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve relevant splits for any question using similarity search.\n",
    "\n",
    "# This is simply \"top K\" retrieval where we select documents based on embedding similarity to the query.\n",
    "\n",
    "TOP_K = 5\n",
    "\n",
    "docs = vectorstore.similarity_search(QUESTION, k=TOP_K)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2940dfc1-cc62-4778-a89d-81da8a466980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Model built by the company Cohere with a vast amount of information to help assist and engage in conversations with users. I am designed to generate human-like responses and assist with a variety of tasks.Okay, well you should try to be simpler. Imagine that we‚Äôre talking to a boy, let‚Äôs say, around eleven years old. What is a Large Language Model (LLM)? Could you give some examples of what can we do with a LLM?Sure, let‚Äôs simplify things with Zachary.You can think of a Large Language Model as a very', metadata={'description': 'Well, explaining AI is not easy, especially for non-technical people. But, nevertheless, we should try! So, I thought it was a good idea to speak directly to one of the protagonists of this field‚Ä¶', 'language': 'en', 'source': 'https://medium.com/the-modern-scientist/a-conversation-around-generative-ai-cca5c408ead4', 'title': 'A conversation around Generative AI | by Luigi Saetta | The Modern Scientist | Sep, 2023 | Medium'}),\n",
       " Document(page_content='Generative AI: how to control the creativity of your Large Language Model | by Luigi Saetta | Oct, 2023 | MediumGenerative AI: how to control the creativity of your Large Language ModelLuigi Saetta¬∑Follow4 min read¬∑Oct 7--ListenSharePhoto by Joshua Hoehne on UnsplashAt the heart of every Generative Service for Text, there is a Large Language Model (LLM), a model that has been trained, on a very large corpus of documents, to learn the structure and characteristics of languages and specifically has been', metadata={'description': 'At the heart of every Generative Service for Text, there is a Large Language Model (LLM), a model that has been trained, on a very large corpus of documents, to learn the structure and‚Ä¶', 'language': 'en', 'source': 'https://luigi-saetta.medium.com/generative-ai-how-to-control-the-creativity-of-your-large-language-model-c7b0322b4c3d', 'title': 'Generative AI: how to control the creativity of your Large Language Model | by Luigi Saetta | Oct, 2023 | Medium'}),\n",
       " Document(page_content='text generation is trained to generate a text, in other words, a sequence of tokens (a word can be split into several tokens, for example: water-fall). A text is given as input to the model and it is called a prompt. It can contain some context information plus, for example, a question. The model produces as output a text that should contain the correct answer to the question. This output is called completion.The current architecture that is commonly used today for LLM is the Transformer (see, for example,', metadata={'description': 'At the heart of every Generative Service for Text, there is a Large Language Model (LLM), a model that has been trained, on a very large corpus of documents, to learn the structure and‚Ä¶', 'language': 'en', 'source': 'https://luigi-saetta.medium.com/generative-ai-how-to-control-the-creativity-of-your-large-language-model-c7b0322b4c3d', 'title': 'Generative AI: how to control the creativity of your Large Language Model | by Luigi Saetta | Oct, 2023 | Medium'}),\n",
       " Document(page_content='control the variety of possible answers to be produced. Maybe, I‚Äôll explore them in another article.If you want to try the OCI Generative AI Service and its Python API, you can participate in the OCI AI beta program. More details here.You can get more information and details on Large Language Models, for example, in Coursera training Generative AI with Large Language ModelsAIGenerative Ai Use CasesLarge Language Models----FollowWritten by Luigi Saetta130 FollowersBorn in the wonderful city of Naples, but', metadata={'description': 'At the heart of every Generative Service for Text, there is a Large Language Model (LLM), a model that has been trained, on a very large corpus of documents, to learn the structure and‚Ä¶', 'language': 'en', 'source': 'https://luigi-saetta.medium.com/generative-ai-how-to-control-the-creativity-of-your-large-language-model-c7b0322b4c3d', 'title': 'Generative AI: how to control the creativity of your Large Language Model | by Luigi Saetta | Oct, 2023 | Medium'}),\n",
       " Document(page_content='been surprised by the quality of the answers that a Large Language Model, like Cohere Command, is able to generate. Therefore, it should be easy to understand what kind of contribution, almost in any field, can be given using such models.As you can easily test by yourself, using for example Cohere Playground, you can actually get the answers reported here using the questions, in bold, as prompt.----FollowWritten by Luigi Saetta130 Followers¬∑Writer for The Modern ScientistBorn in the wonderful city of', metadata={'description': 'Well, explaining AI is not easy, especially for non-technical people. But, nevertheless, we should try! So, I thought it was a good idea to speak directly to one of the protagonists of this field‚Ä¶', 'language': 'en', 'source': 'https://medium.com/the-modern-scientist/a-conversation-around-generative-ai-cca5c408ead4', 'title': 'A conversation around Generative AI | by Luigi Saetta | The Modern Scientist | Sep, 2023 | Medium'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452bd3e-2721-4cc8-892e-4f520b76b7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3107a72-ae17-4652-ab4c-f82fc5113d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
