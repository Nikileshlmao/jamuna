{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b9f0b37-fbfe-4e59-ab32-da1c1afe3b00",
   "metadata": {},
   "source": [
    "* see: https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75812374-a9a1-4463-93a2-6dd6f352cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# removed OPenAI, using HF\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "# removed OpenAI, using Cohere\n",
    "from langchain.chat_models import ChatCohere\n",
    "\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988c589-f06e-49f7-b23f-f70ab316e720",
   "metadata": {},
   "source": [
    "#### Loading the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5edf3f-17df-4963-82ef-d4297239dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOG_POST = \"https://luigi-saetta.medium.com/generative-ai-how-to-control-the-creativity-of-your-large-language-model-c7b0322b4c3d\"\n",
    "\n",
    "loader = WebBaseLoader(BLOG_POST)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd8887b-0aff-45b8-9df2-ea904815ea26",
   "metadata": {},
   "source": [
    "#### Splitting the document in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f53344e-271e-4484-bd29-c95a02b35d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 512\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = CHUNK_SIZE, chunk_overlap = 0)\n",
    "\n",
    "splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89b1b9f1-d375-4a68-b25a-633930c2462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 15 splits...\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {len(splits)} splits...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f70cce26-c603-4532-8ab6-8c34fa4d857e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='trained to generate text.In this article, I’ll discuss some general characteristics of LLM for text generation. Especially those parameters that can be used, in the inference phase, to control the variety of possible answers to prompts. Then, I’ll add some more details regarding Oracle OCI AI Generative Service, which is based on LLMs provided by Cohere.First of all, some basic concepts: how does an LLM generating text work, what is a prompt, and what does it mean that an LLM can be creative?An LLM for', metadata={'source': 'https://luigi-saetta.medium.com/generative-ai-how-to-control-the-creativity-of-your-large-language-model-c7b0322b4c3d', 'title': 'Generative AI: how to control the creativity of your Large Language Model | by Luigi Saetta | Oct, 2023 | Medium', 'description': 'At the heart of every Generative Service for Text, there is a Large Language Model (LLM), a model that has been trained, on a very large corpus of documents, to learn the structure and…', 'language': 'en'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a63cc21-2225-4e1f-8b3a-6ec41ca97d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have substitued OpenAI with HF\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL_NAME,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# using Chroma as Vector store\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=hf)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aaa4a5-ba64-4222-a65c-d26e5395cd3f",
   "metadata": {},
   "source": [
    "#### Define the prompt structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d2dd3-ac41-4cb2-a975-b35ca482fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2ef0b-f63b-4138-8bd9-5271e66ee926",
   "metadata": {},
   "source": [
    "#### Define the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f0d42-8203-4950-a5ca-8c051ff71b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"8K74xpSLRe7Fv3mNPLeLWKTIdReVNlL13YXlogg3\"\n",
    "\n",
    "llm = ChatCohere(cohere_api_key=API_KEY, temperature=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe0dca-b88e-4c5f-9b03-25da1ed36cb2",
   "metadata": {},
   "source": [
    "#### Define the (Lang)Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93a1de-9939-4120-8910-66d735de7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
    "    | rag_prompt \n",
    "    | llm \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce71ae-50a8-418d-81ea-cea0a30d11cf",
   "metadata": {},
   "source": [
    "#### Process the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069689c8-4786-455b-aee0-91f1fc69e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"What is the architecture used for LLM?\"\n",
    "\n",
    "response = rag_chain.invoke(QUESTION)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93ffe1-3036-4251-9514-0c3cc240f224",
   "metadata": {},
   "source": [
    "#### Explore the vectore store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055f1c4-27d9-465a-b0b4-278ad06bc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant splits for any question using similarity search.\n",
    "\n",
    "# This is simply \"top K\" retrieval where we select documents based on embedding similarity to the query.\n",
    "\n",
    "TOP_K = 5\n",
    "\n",
    "docs = vectorstore.similarity_search(QUESTION, \n",
    "                                    k = TOP_K)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940dfc1-cc62-4778-a89d-81da8a466980",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452bd3e-2721-4cc8-892e-4f520b76b7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
