{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b9f0b37-fbfe-4e59-ab32-da1c1afe3b00",
   "metadata": {},
   "source": [
    "* see: https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75812374-a9a1-4463-93a2-6dd6f352cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# two possible vector store\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# removed OpenAI, using HF\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "# removed OpenAI, using OCI GenAI\n",
    "import oci\n",
    "\n",
    "# oci_llm is in a local file\n",
    "from oci_llm import OCIGenAILLM\n",
    "\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaede75f-e609-4cc6-a077-259b3a439bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to enable some debugging\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef483ade-2146-4218-8130-95abe1607a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read OCI config to connect to OCI with API key\n",
    "CONFIG_PROFILE = \"DEFAULT\"\n",
    "config = oci.config.from_file(\"~/.oci/config\", CONFIG_PROFILE)\n",
    "\n",
    "# OCI GenAI endpoint (for now Chicago)\n",
    "ENDPOINT = \"https://generativeai.aiservice.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "# check the config to access to api keys\n",
    "if DEBUG:\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988c589-f06e-49f7-b23f-f70ab316e720",
   "metadata": {},
   "source": [
    "#### Loading the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5edf3f-17df-4963-82ef-d4297239dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOG_POST = \"https://python.langchain.com/docs/get_started/introduction\"\n",
    "loader = WebBaseLoader(BLOG_POST)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd8887b-0aff-45b8-9df2-ea904815ea26",
   "metadata": {},
   "source": [
    "#### Splitting the document in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f53344e-271e-4484-bd29-c95a02b35d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 512\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\n",
    "\n",
    "splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b1b9f1-d375-4a68-b25a-633930c2462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 8 splits...\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {len(splits)} splits...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f70cce26-c603-4532-8ab6-8c34fa4d857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at a single split\n",
    "if DEBUG:\n",
    "    print(splits[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4830653-e501-4dda-a26e-8faef61a0531",
   "metadata": {},
   "source": [
    "#### Embeddings and Vectore Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a63cc21-2225-4e1f-8b3a-6ec41ca97d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have substituted OpenAI with HF\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "\n",
    "\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL_NAME, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# using Chroma or FAISS as Vector store\n",
    "# vectorstore = Chroma.from_documents(documents=splits,\n",
    "#                                    embedding=hf)\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=hf)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aaa4a5-ba64-4222-a65c-d26e5395cd3f",
   "metadata": {},
   "source": [
    "#### Define the prompt structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d2dd3-ac41-4cb2-a975-b35ca482fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2ef0b-f63b-4138-8bd9-5271e66ee926",
   "metadata": {},
   "source": [
    "#### Define the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f0d42-8203-4950-a5ca-8c051ff71b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARTMENT_OCID = \"ocid1.compartment.oc1..aaaaaaaag2cpni5qj6li5ny6ehuahhepbpveopobooayqfeudqygdtfe6h3a\"\n",
    "\n",
    "llm = OCIGenAILLM(\n",
    "    temperature=1,\n",
    "    config=config,\n",
    "    compartment_id=COMPARTMENT_OCID,\n",
    "    endpoint=ENDPOINT,\n",
    "    debug=DEBUG,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe0dca-b88e-4c5f-9b03-25da1ed36cb2",
   "metadata": {},
   "source": [
    "#### Define the (Lang)Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93a1de-9939-4120-8910-66d735de7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | rag_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce71ae-50a8-418d-81ea-cea0a30d11cf",
   "metadata": {},
   "source": [
    "#### Process the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5562c9d-cc2a-4761-9625-1c62a45cf036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of possible questions\n",
    "QUESTION1 = \"What is the best architecture for an LLM?\"\n",
    "QUESTION2 = \"What is LangChain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069689c8-4786-455b-aee0-91f1fc69e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = rag_chain.invoke(QUESTION2)\n",
    "\n",
    "print(\"The response:\")\n",
    "print(response)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93ffe1-3036-4251-9514-0c3cc240f224",
   "metadata": {},
   "source": [
    "#### Explore the vectore store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055f1c4-27d9-465a-b0b4-278ad06bc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant splits for any question using similarity search.\n",
    "\n",
    "# This is simply \"top K\" retrieval where we select documents based on embedding similarity to the query.\n",
    "\n",
    "TOP_K = 5\n",
    "\n",
    "docs = vectorstore.similarity_search(QUESTION2, k=TOP_K)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940dfc1-cc62-4778-a89d-81da8a466980",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452bd3e-2721-4cc8-892e-4f520b76b7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
