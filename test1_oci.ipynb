{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b9f0b37-fbfe-4e59-ab32-da1c1afe3b00",
   "metadata": {},
   "source": [
    "* see: https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75812374-a9a1-4463-93a2-6dd6f352cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# two possible vector store\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# removed OpenAI, using HF\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "# removed OpenAI, using OCI GenAI\n",
    "import oci\n",
    "\n",
    "# oci_llm is in a local file\n",
    "from oci_llm import OCIGenAILLM\n",
    "\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaede75f-e609-4cc6-a077-259b3a439bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to enable some debugging\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef483ade-2146-4218-8130-95abe1607a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read OCI config to connect to OCI with API key\n",
    "CONFIG_PROFILE = \"DEFAULT\"\n",
    "config = oci.config.from_file(\"~/.oci/config\", CONFIG_PROFILE)\n",
    "\n",
    "# OCI GenAI endpoint (for now Chicago)\n",
    "ENDPOINT = \"https://generativeai.aiservice.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "# check the config to access to api keys\n",
    "if DEBUG:\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988c589-f06e-49f7-b23f-f70ab316e720",
   "metadata": {},
   "source": [
    "#### Loading the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5edf3f-17df-4963-82ef-d4297239dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOG_POST = \"https://python.langchain.com/docs/get_started/introduction\"\n",
    "loader = WebBaseLoader(BLOG_POST)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd8887b-0aff-45b8-9df2-ea904815ea26",
   "metadata": {},
   "source": [
    "#### Splitting the document in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f53344e-271e-4484-bd29-c95a02b35d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 512\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\n",
    "\n",
    "splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b1b9f1-d375-4a68-b25a-633930c2462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 8 splits...\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {len(splits)} splits...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f70cce26-c603-4532-8ab6-8c34fa4d857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at a single split\n",
    "if DEBUG:\n",
    "    print(splits[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4830653-e501-4dda-a26e-8faef61a0531",
   "metadata": {},
   "source": [
    "#### Embeddings and Vectore Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a63cc21-2225-4e1f-8b3a-6ec41ca97d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have substituted OpenAI with HF\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "\n",
    "\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL_NAME, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# using Chroma or FAISS as Vector store\n",
    "# vectorstore = Chroma.from_documents(documents=splits,\n",
    "#                                    embedding=hf)\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=hf)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aaa4a5-ba64-4222-a65c-d26e5395cd3f",
   "metadata": {},
   "source": [
    "#### Define the prompt structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "426d2dd3-ac41-4cb2-a975-b35ca482fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2ef0b-f63b-4138-8bd9-5271e66ee926",
   "metadata": {},
   "source": [
    "#### Define the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d5f0d42-8203-4950-a5ca-8c051ff71b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARTMENT_OCID = \"ocid1.compartment.oc1..aaaaaaaag2cpni5qj6li5ny6ehuahhepbpveopobooayqfeudqygdtfe6h3a\"\n",
    "\n",
    "llm = OCIGenAILLM(\n",
    "    temperature=1,\n",
    "    config=config,\n",
    "    compartment_id=COMPARTMENT_OCID,\n",
    "    endpoint=ENDPOINT,\n",
    "    debug=DEBUG,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe0dca-b88e-4c5f-9b03-25da1ed36cb2",
   "metadata": {},
   "source": [
    "#### Define the (Lang)Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e93a1de-9939-4120-8910-66d735de7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | rag_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce71ae-50a8-418d-81ea-cea0a30d11cf",
   "metadata": {},
   "source": [
    "#### Process the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5562c9d-cc2a-4761-9625-1c62a45cf036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of possible questions\n",
    "QUESTION1 = \"What is the best architecture for an LLM?\"\n",
    "QUESTION2 = \"What is LangChain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "069689c8-4786-455b-aee0-91f1fc69e0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response:\n",
      " LangChain is a framework for developing applications powered by language models. It enables applications that are easy to customize, and build new ones.\n",
      "\n",
      "CPU times: user 115 ms, sys: 38.7 ms, total: 154 ms\n",
      "Wall time: 5.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = rag_chain.invoke(QUESTION2)\n",
    "\n",
    "print(\"The response:\")\n",
    "print(response)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93ffe1-3036-4251-9514-0c3cc240f224",
   "metadata": {},
   "source": [
    "#### Explore the vectore store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e055f1c4-27d9-465a-b0b4-278ad06bc67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve relevant splits for any question using similarity search.\n",
    "\n",
    "# This is simply \"top K\" retrieval where we select documents based on embedding similarity to the query.\n",
    "\n",
    "TOP_K = 5\n",
    "\n",
    "docs = vectorstore.similarity_search(QUESTION2, k=TOP_K)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2940dfc1-cc62-4778-a89d-81da8a466980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "framework or notOff-the-shelf chains: a structured assembly of components for accomplishing specific higher-level tasksOff-the-shelf chains make it easy to get started. For complex applications, components make it easy to customize existing chains and build new ones.Get started‚ÄãHere‚Äôs how to install LangChain, set up your environment, and start building.We recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.Note: These docs\n",
      "\n",
      "are for the LangChain Python package. For documentation on LangChain.js, the JS/TS version, head here.Modules‚ÄãLangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:Model I/O‚ÄãInterface with language modelsRetrieval‚ÄãInterface with application-specific dataChains‚ÄãConstruct sequences of callsAgents‚ÄãLet chains choose which tools to use given high-level directivesMemory‚ÄãPersist application state between runs of a\n",
      "\n",
      "Skip to main contentü¶úÔ∏èüîó LangChainDocsUse casesIntegrationsAPICommunityChat our docsLangSmithJS/TS DocsSearchCTRLKGet startedIntroductionInstallationQuickstartLangChain Expression LanguageInterfaceHow toCookbookLangChain Expression Language (LCEL)Why use LCEL?ModulesModel I/‚ÄãORetrievalChainsMemoryAgentsCallbacksModulesSecurityGuidesMoreGet startedIntroductionOn this pageIntroductionLangChain is a framework for developing applications powered by language models. It enables applications that:Are\n",
      "\n",
      "chainCallbacks‚ÄãLog and stream intermediate steps of any chainExamples, ecosystem, and resources‚ÄãUse cases‚ÄãWalkthroughs and best-practices for common end-to-end use cases, like:Document question answeringChatbotsAnalyzing structured dataand much more...Guides‚ÄãLearn best practices for developing with LangChain.Ecosystem‚ÄãLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations and dependent repos.Additional\n",
      "\n",
      "methods in the LangChain Python package.PreviousGet startedNextInstallationGet startedModulesExamples, ecosystem, and resourcesUse casesGuidesEcosystemAdditional resourcesCommunityAPI referenceCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452bd3e-2721-4cc8-892e-4f520b76b7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228be3c9-9c7b-4501-a2fe-656cdefd0a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
